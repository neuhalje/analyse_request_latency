{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "Analyse (a custom formated) http-request log for request duration.\n",
    "\n",
    "Start with `pipenv run jupyter notebook`.\n",
    "\n",
    "\n",
    "## Example\n",
    "\n",
    "![All requests](README.inc/all_requests.png)\n",
    "\n",
    "![Just operation G](README.inc/operation_g_requests.png)\n",
    "\n",
    "## Data format\n",
    "\n",
    "The example dataset is split into the following columns\n",
    "\n",
    "| Name | format | description |\n",
    "|---|---|---|\n",
    "| Timestamp  | timestamp  | Occurence of the request  |\n",
    "| URL  | string | URL of the request (ignored!)  |\n",
    "| command  | string  | The command executed. This is the grouping criteria for analysis.  |\n",
    "| duration  | int  | Duration  of the request.  |\n",
    "\n",
    "The unit of `duration` can be configured in `DURATION_UNIT`.\n",
    "\n",
    "```csv\n",
    "14/Jan/2019:03:46:03 /example/url operation_G 0\n",
    "14/Jan/2019:03:46:07 /example/url operation_G 0\n",
    "14/Jan/2019:03:46:07 /example/url operation_G 2\n",
    "14/Jan/2019:03:46:08 /example/url operation_G 0\n",
    "14/Jan/2019:03:46:09 /example/url operation_G 0\n",
    "14/Jan/2019:03:46:10 /example/url operation_B 5\n",
    "14/Jan/2019:03:46:19 /example/url operation_A 0\n",
    "14/Jan/2019:03:46:19 /example/url operation_F 90\n",
    "14/Jan/2019:03:46:20 /example/url operation_E 9\n",
    "14/Jan/2019:03:46:24 /example/url operation_F 0\n",
    "```\n",
    "\n",
    "## Contributing\n",
    "\n",
    "I am hosted at [GitHub](https://github.com/neuhalje/analyse_request_latency)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data format\n",
    "\n",
    "The unit of the `duration` column is used as exmpanatory text in various charts. It is not interpreted in any way.\n",
    "\n",
    "```python\n",
    "# The unit of the duration column - only used in labeling charts\n",
    "DURATION_UNIT = \"s\"\n",
    "```\n",
    "\n",
    "\n",
    "### Filtering\n",
    "\n",
    "The source data set can be filtered prior sampling. This is especially useful only a subset of the calls are relevant. E.g. remove longpolling URLs because they would skew the statistics.\n",
    "\n",
    "\n",
    "```python\n",
    "# None: Do not filter at all\n",
    "QUERY_FILTER=None\n",
    "\n",
    "# Filter for all commands named 'myCommand'\n",
    "QUERY_FILTER=\"command == 'myCommand'\"\n",
    "\n",
    "# Do some more elaborated filtering in Python\n",
    "\n",
    "# Remove all urls that contain the string 'Lazy'\n",
    "QUERY_FILTER=lambda df : np.logical_not(df.url.str.match('.*Lazy.*'))\n",
    "\n",
    "# E.g. all commands that do not include 'G' and whose URL starts with ''/example'\n",
    "QUERY_FILTER=lambda df : np.logical_not(df.command.str.match('.*G.*')) & df.url.str.match(r'^/example')\n",
    "```\n",
    "\n",
    "### Sampling\n",
    "\n",
    "After filtering the data is sampled. Meaning: only a subset of the data is used for charting.  This considerably speeds up sampling. Especially the [plotly](https://plot.ly) charts benefit from a reduced dataset as they are rendered in the browser. Depending on your setup a chart with more than 100k elements will come close to freezing your browser.\n",
    "\n",
    "With a large enough sampling the loss of precission os neglectable.\n",
    "\n",
    "```python\n",
    "# 'None' disables sampling.\n",
    "MAX_ELEMENT_COUNT=None\n",
    "\n",
    "# Take at most 100.000 elements.\n",
    "MAX_ELEMENT_COUNT=100_000\n",
    "```\n",
    "\n",
    "### Massage the data\n",
    "\n",
    "Outliers can make charts unusable, especially when they are magnitudes larger than the average data point. For getting an impression of the performance it is often good enough to know when a measurement is 'just too high'.\n",
    "\n",
    "#### y-axis\n",
    "\n",
    "This notebook provides two tunables for outlier filtering for the duration (y-axis):\n",
    "* `PERCENTILE_LIMIT` (e.g. 0.999) sets all value higher than the (in this case 99.9%) percentile to the value of the percentile.\n",
    "* `DURATION_LIMIT` is an additional upper limit.\n",
    "\n",
    "Say `DURATION_LIMIT` is `100` and the calculated percentile of `PERCENTILE_LIMIT` is `101` then all values will be capped at `100`.\n",
    "\n",
    "Say `DURATION_LIMIT` is `120` and the calculated percentile of `PERCENTILE_LIMIT` is `101` then all values will be capped at `101`. \n",
    "\n",
    "```python\n",
    "# Cap y-axis to the 99.9th percentile or 60, whichever is lower.\n",
    "PERCENTILE_LIMIT=0.999\n",
    "DURATION_LIMIT=60  # unit: the same unit as used in the data files\n",
    "```\n",
    "\n",
    "#### x-axis\n",
    "\n",
    "In order to control the resolution of the x-axis data can be grouped in buckets.\n",
    "\n",
    "```python\n",
    "# https://pandas.pydata.org/pandas-docs/stable/timeseries.html#timeseries-offset-aliases\n",
    "ROUND_TO=\"15min\"\n",
    "\n",
    "ROUND_TO=\"6H\"\n",
    "\n",
    "ROUND_TO=\"1min\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_FILTER=lambda df : np.logical_not(df.command.str.match('.*G.*')) & df.url.str.match(r'^/example')\n",
    "PERCENTILE_LIMIT=0.999\n",
    "DURATION_LIMIT=60 \n",
    "ROUND_TO=\"15min\"\n",
    "DURATION_UNIT = \"s\"\n",
    "MAX_ELEMENT_COUNT=100_000\n",
    "\n",
    "_DATASET=\"example_dataset.txt\"\n",
    "\n",
    "#_DATASET=\"../combined-sorted_Jan_31-Feb_06.txt\"\n",
    "#_DATASET=\"../combined-sorted-Jan_14-Jan_22.txt\"\n",
    "\n",
    "DATASET=_DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset():\n",
    "    df = pd.read_csv(DATASET, sep=\" \")\n",
    "    df.columns = ['ts', 'url', 'command', 'duration']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red Tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot,iplot\n",
    "import plotly.plotly as py\n",
    "import plotly.tools as tls\n",
    "\n",
    "from IPython.core.display import display, HTML, Markdown\n",
    "\n",
    "\n",
    "#Always run this the command before at the start of notebook (for Plotly)\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "def configure_figure_size():\n",
    "    matplotlib.rcParams['figure.figsize'] = [15, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "//  This is unsupported but increases the size of the output. Needed to really see the heatmaps\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_dataset()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial datase information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.command.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duration.describe(percentiles=[.25, .5, .75, .9, .95, .99, .999,.9999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duration.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the dataset, take a sampling (for faster processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if QUERY_FILTER:\n",
    "    if isinstance(QUERY_FILTER, str):\n",
    "        df = df.query(QUERY_FILTER)\n",
    "    else:\n",
    "        df = df[QUERY_FILTER]\n",
    "\n",
    "unsampled_dataset_size = len(df.index)\n",
    "\n",
    "if MAX_ELEMENT_COUNT:\n",
    "     df = df.sample(n=min(MAX_ELEMENT_COUNT, len(df.index)))\n",
    "\n",
    "sampled_dataset_size = len(df.index)\n",
    "\n",
    "df.head()\n",
    "print(f\"Sampled dataset down to {100 * sampled_dataset_size / unsampled_dataset_size:_.2f}% of original size ({unsampled_dataset_size:_} elements to {sampled_dataset_size:_} elements).\")\n",
    "\n",
    "sampling = { 'unsampled_dataset_size' : unsampled_dataset_size, \n",
    "             'sampled_dataset_size' : sampled_dataset_size,\n",
    "             'ratio' : sampled_dataset_size / unsampled_dataset_size\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data\n",
    "\n",
    "You can customize the format of the timestamp here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timestamp'] = pd.to_datetime(df.ts,format=\"%d/%b/%Y:%H:%M:%S\")\n",
    "\n",
    "# put all requests in bins (e.g. 15min bins)\n",
    "df['approx_ts'] = df['timestamp'].dt.round(ROUND_TO)  \n",
    "df.sort_values(by='timestamp', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clip outliers\n",
    "\n",
    "Two bounds are put on the lateny:\n",
    "* an absolute bound of `DURATION_LIMIT` seconds\n",
    "* the `PERCENTILE_LIMIT`  (e.g. 0.999) which is calculated from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duration.describe(percentiles=[.25, .5, .75, .9, .95, .99, .999,.9999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_dataset(df):\n",
    "    q = df.duration.quantile(q=PERCENTILE_LIMIT)\n",
    "    latency_clipped_at = min(DURATION_LIMIT, q)\n",
    "    df.duration.clip(upper=latency_clipped_at, inplace=True)\n",
    "    return latency_clipped_at\n",
    "\n",
    "latency_clipped_at = clip_dataset(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a counter column\n",
    "This is used in aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['count'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the most costly calls\n",
    "\n",
    "This calculation is based on the filtered (command), clipped (outliers) and sampled (number of items) dataset. In any case it should give a good indication which calls should be considered for optimisations.\n",
    "\n",
    "Depending on the unit of the latency the averages can be misleading, esp. if the unit is [s] and all calls <500ms are logged as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_aggregated_call_cost(df):\n",
    "    grouped = df.groupby(['command'], as_index=False)\n",
    "    aggregated = grouped['duration'].agg([np.size, np.sum, np.average], as_index=False)\n",
    "    aggregated.sort_values('sum', inplace=True, ascending=False)\n",
    "    return aggregated\n",
    "\n",
    "def plot_aggregated_call_cost(aggregated_call_cost):\n",
    "    text_labels = [ f\"\"\"{command}: {count:_.0f} calls totalling in {sum:_.0f}{DURATION_UNIT} w. an average latency of {average:.1f}{DURATION_UNIT}\"\"\"  \n",
    "                   for command, sum, count, average in  \n",
    "                       zip(aggregated_call_cost.index,\n",
    "                           aggregated_call_cost['sum'],\n",
    "                           aggregated_call_cost['size'],\n",
    "                           aggregated_call_cost['average'])]    \n",
    "    \n",
    "    data = [go.Bar(\n",
    "            x=aggregated_call_cost.index,\n",
    "            y=aggregated_call_cost['sum'],\n",
    "            text  = text_labels\n",
    "    )]\n",
    "\n",
    "    layout= go.Layout(\n",
    "            title= \"Absolute call cost per command\",\n",
    "            hovermode= 'closest',\n",
    "            xaxis= dict(\n",
    "                title= 'Operation',\n",
    "                ticklen= 5,\n",
    "                zeroline= False,\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title= f'Aggregated call time in [{DURATION_UNIT}]',\n",
    "            ticklen= 5,\n",
    "            gridwidth= 2,\n",
    "        ),\n",
    "        showlegend= False\n",
    "    )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    iplot(fig)\n",
    "    \n",
    "#calculate_aggregated_call_cost(df).head()  \n",
    "plot_aggregated_call_cost(calculate_aggregated_call_cost(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.timestamp.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the distribution of  latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution_of_duration(df):\n",
    "    configure_figure_size()\n",
    "    sns.distplot(df.duration, kde=False)\n",
    "    \n",
    "distribution_of_duration(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latency Heatmap\n",
    "\n",
    "Show latency as a heatmap with time on the x-axis, latency on the y-axis and color frequency of measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def aggregate_for_heatmap(df):\n",
    "    grouped = df.groupby(['approx_ts','duration'], as_index=False)\n",
    "    aggregated = grouped['count'].agg(np.sum)\n",
    "    # approx_count contains the expected number of calls based on the sampling rate\n",
    "    # it is a best guess on very inaccurate for small values of 'count'.\n",
    "    # Also this accumulates rounding/floating point errors: summing up approx_count\n",
    "    # will likely NOT yield the same value as first summing the 'count' value and then\n",
    "    # multiplying by 'ratio'\n",
    "    aggregated['approx_count'] = (aggregated['count'] / sampling['ratio']).astype(int)\n",
    "    return aggregated\n",
    "    \n",
    "def plot_latency_heatmap(aggregated, command):\n",
    "    call_count = aggregated['approx_count'].sum()\n",
    "    \n",
    "    if command:\n",
    "        title = f'Latency of ~{call_count:_} \"{command}\" calls in [{DURATION_UNIT}]'\n",
    "    else:\n",
    "        # not filtered\n",
    "        title = f'Latency of ~{call_count:_} calls in [{DURATION_UNIT}]'\n",
    "\n",
    "    #\n",
    "    #  Call count\n",
    "    #\n",
    "    \n",
    "    approx_call_count_by_date = aggregated[['approx_ts','approx_count']].groupby(['approx_ts'], as_index = False)['approx_count'].agg('sum')\n",
    "    \n",
    "    text_labels_count  = [ f\"\"\"{when} - ~{count:_} calls total\"\"\"  \n",
    "                   for when, count in  \n",
    "                       zip(approx_call_count_by_date['approx_ts'],\n",
    "                           approx_call_count_by_date['approx_count'])]\n",
    "                          \n",
    "    call_count_trace = go.Scatter(\n",
    "        name=\"call count\",\n",
    "        x = approx_call_count_by_date['approx_ts'],\n",
    "        y = approx_call_count_by_date['approx_count'],\n",
    "        text  = text_labels_count,\n",
    "        mode = 'lines+markers',\n",
    "        yaxis='y',\n",
    "        line = dict(color = \"orange\")\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # latency heatmap \n",
    "    #\n",
    "    \n",
    "    text_labels_heatmap = [ f\"\"\"{when} - ~{count:_} call(s) w. {latency:_.1f}{DURATION_UNIT} latency\"\"\"  \n",
    "                   for when, latency,count in  \n",
    "                       zip(aggregated['approx_ts'],\n",
    "                           aggregated['duration'],\n",
    "                           aggregated['approx_count'])\n",
    "                  ]\n",
    "\n",
    "    heatmap_trace = go.Scatter(\n",
    "        name=\"latency distribution\",\n",
    "        x = aggregated['approx_ts'],\n",
    "        y = aggregated['duration'],\n",
    "        text  = text_labels_heatmap,\n",
    "        mode ='markers',\n",
    "        marker=dict(\n",
    "            color = aggregated['approx_count'],\n",
    "            colorscale = 'Jet',\n",
    "            showscale = True,\n",
    "            symbol = \"square\",\n",
    "            colorbar = dict(\n",
    "                x=1.07\n",
    "            )\n",
    "        ),\n",
    "        yaxis='y2'\n",
    "    )\n",
    "      \n",
    "\n",
    "\n",
    "    #\n",
    "    #  Layout\n",
    "    #\n",
    "    \n",
    "    layout = go.Layout(\n",
    "        title = title,\n",
    "        hovermode = 'closest',\n",
    "       \n",
    "        xaxis = dict(\n",
    "                title = 'Timestamp',\n",
    "                ticklen = 5,\n",
    "                zeroline = True,\n",
    "                gridwidth = 2,\n",
    "        ),\n",
    "\n",
    "        yaxis = dict(\n",
    "            title = f'Call count',\n",
    "            side='right',\n",
    "        ),\n",
    "         yaxis2 = dict(\n",
    "            title = f'Latency in [{DURATION_UNIT}] (capped at {latency_clipped_at:_.1f}{DURATION_UNIT})',\n",
    "            side='left',\n",
    "            overlaying='y'),\n",
    "        showlegend = False,\n",
    "        #paper_bgcolor='rgba(0,0,0,255)',\n",
    "        plot_bgcolor='rgba(64,64,64,255)'\n",
    "    )\n",
    "\n",
    "    data = [call_count_trace,heatmap_trace]\n",
    "    fig = go.Figure(data = data, layout = layout)\n",
    "\n",
    "    iplot(fig)\n",
    "\n",
    "    display(aggregated.duration.describe(percentiles=[.25, .5, .75, .9, .95, .99, .999,.9999]))\n",
    "\n",
    "def describe_command(aggregated, command):\n",
    "    if command:\n",
    "        title = f'{command}'\n",
    "    else:\n",
    "        # not filtered\n",
    "        title = f'all commands'\n",
    "    call_count = aggregated['count'].sum()\n",
    "    display(Markdown(f\"\"\"\n",
    "## {title}\n",
    "\n",
    "Call duration of {title}. The sampled dataset contains **{call_count:_} {title} calls**. \n",
    "Based on the sampling rate the original dataset is expected to contain **~{call_count / sampling['ratio']:_.0f} calls**.\n",
    "That means that **~{100 * call_count / sampling['sampled_dataset_size']:_.0f}%** of all calls are {title} calls.\n",
    "\n",
    "Due to rounding or floating point errors the values in the chart do not sum up accurately.\n",
    "\"\"\"))\n",
    "\n",
    "    plot_latency_heatmap(aggregated, command) \n",
    "\n",
    "describe_command(aggregate_for_heatmap(df), command = None)\n",
    "\n",
    "commands_by_calls = df['command'].value_counts().sort_values(ascending = False)\n",
    "for command in commands_by_calls.index:\n",
    "    call_count = commands_by_calls[command]\n",
    "    filtered_df = df.query(\"command == @command\")\n",
    "    filtered_aggregate = aggregate_for_heatmap(filtered_df)\n",
    "    describe_command(filtered_aggregate, command)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
